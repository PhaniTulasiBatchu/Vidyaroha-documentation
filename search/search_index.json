{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"body{ background-color:rgb(34, 32, 32); } Smart Attendance System Using Facial Recognition AIM Marking of attendance for students captured under a group image. The group images are processed for image correction and amplification of facial features, after which they are matched with the existing database and attendance are marked accordingly. ----TABLE OF CONTENTS---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Home"},{"location":"#smart-attendance-system-using-facial-recognition","text":"","title":"Smart Attendance System Using Facial Recognition"},{"location":"#aim","text":"Marking of attendance for students captured under a group image. The group images are processed for image correction and amplification of facial features, after which they are matched with the existing database and attendance are marked accordingly.","title":"AIM"},{"location":"#-table-of-contents-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENTS----"},{"location":"data-pre-processing/","text":"body{ background-color:rgb(34, 32, 32); } What is DATA PRE- PROCESSING ? Data Pre-Processing includes techniques that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, noisy and lacks certain behaviours or attributes, and is likely to contain many errors and imperfections. So, data pre-processing is a proven method of resolving such issues. The Data we are having in this project is in the form of images. So, we will be dealing with image pre-processing techniques, which are sub-sets of data pre-processing. Image pre-processing includes the steps taken to format the images before they are loaded into training models which includes, image resizing, scaling, colour corrections and many more. Why do we need it ? The data that we receive might not always be in the best form to feed in to the model. Hence to just manipulate the data to the form we can use effectively, we perform data pre-processing. Two things play a very important role in the face recognition pipeline. The first being the pose and second is the lighting conditions (illumination). Problem identified in the dataset given to us: The images had different lighting conditions. When the pipeline would be deployed, we still have no control over the illumination. Hence we need to pre process our data so that all of the images would come to a common lighting condition. The techniques we tried for achieving the same are: * Histogram equalizer * Adaptive Histogram equalizer (CLAHE) for grayscale images * CLAHE - by extracting each of the channels and equalising each channel * CLAHE - by equalising the \u2018V\u2019 channel of the HSV form of the image. * Logarithmic transform * Gamma Correction * Image specific Gamma Correction Concepts Explored 1. Histogram Equalizer Histogram Equalization is a technique used to improve the contrast in images. The regions of lower contrast are improved to a higher contrast. The main objective of Histogram equalizer is to normalise the varying illumination conditions. Histogram Equalization can be applied to the whole image by equalizing the individual channels or by converting the image to another colour space. Adaptive Histogram Equalizer or CLAHE is used to equalize the image at every section. We can set a particular clip limit value according to which the image will be normalized. For this dataset we have used CLAHE by equalizing the \u2018V\u2019 channel of the HSV form of the image. Converting to HSV and then equalising gave the best results as compared to grayscale or RGB. 2. Gamma Correction Gamma correction is used to improve the contrast of an image based on its current lighting conditions, which implies that for each image we can apply a different value of gamma and normalize it. We classified the images into three categories based on the illumination Overexposed : Images with very bright lighting condition Underexposed : Images with dark lighting condition Neutral : Images with a good lighting Based on this classification we used different values of gamma to correct the images. We classified the images based on the bright and dark thresholds and applied the gamma values as 2 for underexposed images and 0.3 for overexposed images. No gamma correction was done for neutral images. 3. Weiner Filter Another major problem that we tend to face while using a dataset that includes images captured manually is Motion Blur The most important technique for removal of blur in images due to linear motion or unfocussed optics is the Wiener filter. The Wiener filtering is a restoration technique for deconvolution that can be used effectively when the frequency characteristics of the image and additive noise are known, to at least some degree, it is possible to recover the image by generalized inverse filtering. The Wiener filtering is optimal in terms of the mean square error. In other words, it minimizes the overall mean square error in the process of inverse filtering and noise smoothing. The Wiener filtering is a linear estimation of the original image. It should be noted that Weiner filters are far and away the most common deblurring technique used because it mathematically returns the best results. It should also be re-emphasized that Wiener filtering is in fact the underlying premise for restoration of other kinds of blur; and being a least-mean-squares technique, it has roots in a spectrum of other engineering applications. OUTPUT:- Concluding Remarks As per our requirements we have used Histogram Equalizer in our pipeline. References refer Contributors 1. Pendyala Sri Harshita 2. Solipuram Akshith Reddy 3. Phani Tulasi Batchu ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Data pre processing"},{"location":"data-pre-processing/#what-is-data-pre-processing","text":"Data Pre-Processing includes techniques that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, noisy and lacks certain behaviours or attributes, and is likely to contain many errors and imperfections. So, data pre-processing is a proven method of resolving such issues. The Data we are having in this project is in the form of images. So, we will be dealing with image pre-processing techniques, which are sub-sets of data pre-processing. Image pre-processing includes the steps taken to format the images before they are loaded into training models which includes, image resizing, scaling, colour corrections and many more.","title":"What is DATA PRE- PROCESSING ?"},{"location":"data-pre-processing/#why-do-we-need-it","text":"The data that we receive might not always be in the best form to feed in to the model. Hence to just manipulate the data to the form we can use effectively, we perform data pre-processing. Two things play a very important role in the face recognition pipeline. The first being the pose and second is the lighting conditions (illumination). Problem identified in the dataset given to us: The images had different lighting conditions. When the pipeline would be deployed, we still have no control over the illumination. Hence we need to pre process our data so that all of the images would come to a common lighting condition. The techniques we tried for achieving the same are: * Histogram equalizer * Adaptive Histogram equalizer (CLAHE) for grayscale images * CLAHE - by extracting each of the channels and equalising each channel * CLAHE - by equalising the \u2018V\u2019 channel of the HSV form of the image. * Logarithmic transform * Gamma Correction * Image specific Gamma Correction","title":"Why do we need it ?"},{"location":"data-pre-processing/#concepts-explored","text":"","title":"Concepts Explored"},{"location":"data-pre-processing/#1-histogram-equalizer","text":"Histogram Equalization is a technique used to improve the contrast in images. The regions of lower contrast are improved to a higher contrast. The main objective of Histogram equalizer is to normalise the varying illumination conditions. Histogram Equalization can be applied to the whole image by equalizing the individual channels or by converting the image to another colour space. Adaptive Histogram Equalizer or CLAHE is used to equalize the image at every section. We can set a particular clip limit value according to which the image will be normalized. For this dataset we have used CLAHE by equalizing the \u2018V\u2019 channel of the HSV form of the image. Converting to HSV and then equalising gave the best results as compared to grayscale or RGB.","title":"1. Histogram Equalizer "},{"location":"data-pre-processing/#2-gamma-correction","text":"Gamma correction is used to improve the contrast of an image based on its current lighting conditions, which implies that for each image we can apply a different value of gamma and normalize it. We classified the images into three categories based on the illumination Overexposed : Images with very bright lighting condition Underexposed : Images with dark lighting condition Neutral : Images with a good lighting Based on this classification we used different values of gamma to correct the images. We classified the images based on the bright and dark thresholds and applied the gamma values as 2 for underexposed images and 0.3 for overexposed images. No gamma correction was done for neutral images.","title":"2. Gamma Correction"},{"location":"data-pre-processing/#3-weiner-filter","text":"Another major problem that we tend to face while using a dataset that includes images captured manually is Motion Blur The most important technique for removal of blur in images due to linear motion or unfocussed optics is the Wiener filter. The Wiener filtering is a restoration technique for deconvolution that can be used effectively when the frequency characteristics of the image and additive noise are known, to at least some degree, it is possible to recover the image by generalized inverse filtering. The Wiener filtering is optimal in terms of the mean square error. In other words, it minimizes the overall mean square error in the process of inverse filtering and noise smoothing. The Wiener filtering is a linear estimation of the original image. It should be noted that Weiner filters are far and away the most common deblurring technique used because it mathematically returns the best results. It should also be re-emphasized that Wiener filtering is in fact the underlying premise for restoration of other kinds of blur; and being a least-mean-squares technique, it has roots in a spectrum of other engineering applications. OUTPUT:-","title":"3. Weiner Filter"},{"location":"data-pre-processing/#concluding-remarks","text":"As per our requirements we have used Histogram Equalizer in our pipeline.","title":"Concluding Remarks"},{"location":"data-pre-processing/#references","text":"refer","title":"References"},{"location":"data-pre-processing/#contributors","text":"1. Pendyala Sri Harshita 2. Solipuram Akshith Reddy 3. Phani Tulasi Batchu","title":"Contributors"},{"location":"data-pre-processing/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"face-alignment/","text":"body{ background-color:rgb(34, 32, 32); } What is Face Alignment Face alignment is the process of identifying the geometric structure of different faces in digital images. We attempt to obtain a canonical alignment of the face based on translation, scaling, and rotation. There are many ways to perform face alignment. Some methods try to impose a (pre-defined) 3D model and then apply a transform to the input image such that the landmarks on the input face match the landmarks on the 3D model. Face alignment can be seen as a form of \u201cdata normalization\u201d. Just as we normalize a set of feature vectors via zero cantering or scaling to unit norm, prior to training any model, we align the faces in our dataset before forwarding them in the pipeline. This process ensures higher accuracy from the face recognition models. Why do we need it We will be dealing with group images in this project. When a group image is captured, it may include positioning of several people in different angles, trying to face the camera with some degree of rotation in the face and other facial landmarks. In this case, there will be a loss in accuracy of the output due to these unnecessary rotations. So, we need to normalize these rotations in order to obtain better results when we move forward in the pipeline. Concepts Explored 1. Face Alignment Face alignment is an early stage of the modern face recognition pipeline. Google declared that face alignment increases the accuracy of its face recognition model FaceNet from 98.87% to 99.63%. This is almost 1% accuracy improvement. Similar to face detection which is also the earlier stage of the pipeline, we can apply 2D face alignment within OpenCV in Python easily. 2. Frontalization \u201cFrontalization\u201d is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward-facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. But here its observed that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces is preferred. This leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation. Difference between the two The major difference is that in face alignment, we try to align a specific feature of the face , say eyeseither horizontally or vertically as per the users choice.No modifications are done to the structure of the face only the orientation is modified. Whereas in case of frontalization, the orientation is ignored and the modifications are made to the angle of tilt in the face so that any face tilted towards the right or left by any degree is turned back straight in such a way that all the facial features are visible in one frame. Contributors 1. Kodumuru Saketh 2. Phani Tulasi Batchu References ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Face alignment"},{"location":"face-alignment/#what-is-face-alignment","text":"Face alignment is the process of identifying the geometric structure of different faces in digital images. We attempt to obtain a canonical alignment of the face based on translation, scaling, and rotation. There are many ways to perform face alignment. Some methods try to impose a (pre-defined) 3D model and then apply a transform to the input image such that the landmarks on the input face match the landmarks on the 3D model. Face alignment can be seen as a form of \u201cdata normalization\u201d. Just as we normalize a set of feature vectors via zero cantering or scaling to unit norm, prior to training any model, we align the faces in our dataset before forwarding them in the pipeline. This process ensures higher accuracy from the face recognition models.","title":"What is Face Alignment"},{"location":"face-alignment/#why-do-we-need-it","text":"We will be dealing with group images in this project. When a group image is captured, it may include positioning of several people in different angles, trying to face the camera with some degree of rotation in the face and other facial landmarks. In this case, there will be a loss in accuracy of the output due to these unnecessary rotations. So, we need to normalize these rotations in order to obtain better results when we move forward in the pipeline.","title":"Why do we need it"},{"location":"face-alignment/#concepts-explored","text":"","title":"Concepts Explored"},{"location":"face-alignment/#1-face-alignment","text":"Face alignment is an early stage of the modern face recognition pipeline. Google declared that face alignment increases the accuracy of its face recognition model FaceNet from 98.87% to 99.63%. This is almost 1% accuracy improvement. Similar to face detection which is also the earlier stage of the pipeline, we can apply 2D face alignment within OpenCV in Python easily.","title":"1. Face Alignment"},{"location":"face-alignment/#2-frontalization","text":"\u201cFrontalization\u201d is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward-facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. But here its observed that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces is preferred. This leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation.","title":"2. Frontalization"},{"location":"face-alignment/#difference-between-the-two","text":"The major difference is that in face alignment, we try to align a specific feature of the face , say eyeseither horizontally or vertically as per the users choice.No modifications are done to the structure of the face only the orientation is modified. Whereas in case of frontalization, the orientation is ignored and the modifications are made to the angle of tilt in the face so that any face tilted towards the right or left by any degree is turned back straight in such a way that all the facial features are visible in one frame.","title":"Difference between the two"},{"location":"face-alignment/#contributors","text":"1. Kodumuru Saketh 2. Phani Tulasi Batchu","title":"Contributors"},{"location":"face-alignment/#references","text":"","title":"References"},{"location":"face-alignment/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"face-detection/","text":"body{ background-color:rgb(34, 32, 32); } Face Detection In this step we will detect the faces in the given image. There are several models that perform this. Some of them we explored are: 1. OpenCV Haarcascade 2. Keras - CNN (Convolutional Neural Network) 3. Face Recognition using Dlib 4. MTCNN in Python OpenCV Haarcascade It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. Then, it can be used on any image we want to detect faces in. It is well known for being able to detect faces and face parts in an image, but can be trained to detect a vast majority of objects. MTCNN in Python MTCNN or Multi-Task Cascaded Convolutional Neural Network is unquestionably one of the most popular and most accurate face detection tools today. As such, it is based on a Deep learning architecture, it specifically consists of 3 neural networks (P-Net, R-Net, and O-Net) connected in a cascade. Concluding Remarks Out of these we used detecting faces with Facenet. Because it is very effective at detecting faces correctly. From facenet_pytorch we imported MTCNN and we used it to detect faces. After pre-processing the image we sent that image to MTCNN to detect the faces. After detecting the faces we are sending those faces for a feature extraction model to get embeddings. Contributors 1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Phani Tulasi Batchu 4. Pendyala Sri Harshita ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Face detection"},{"location":"face-detection/#face-detection","text":"In this step we will detect the faces in the given image. There are several models that perform this. Some of them we explored are: 1. OpenCV Haarcascade 2. Keras - CNN (Convolutional Neural Network) 3. Face Recognition using Dlib 4. MTCNN in Python","title":"Face Detection"},{"location":"face-detection/#opencv-haarcascade","text":"It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. Then, it can be used on any image we want to detect faces in. It is well known for being able to detect faces and face parts in an image, but can be trained to detect a vast majority of objects.","title":"OpenCV Haarcascade"},{"location":"face-detection/#mtcnn-in-python","text":"MTCNN or Multi-Task Cascaded Convolutional Neural Network is unquestionably one of the most popular and most accurate face detection tools today. As such, it is based on a Deep learning architecture, it specifically consists of 3 neural networks (P-Net, R-Net, and O-Net) connected in a cascade.","title":"MTCNN in Python"},{"location":"face-detection/#concluding-remarks","text":"Out of these we used detecting faces with Facenet. Because it is very effective at detecting faces correctly. From facenet_pytorch we imported MTCNN and we used it to detect faces. After pre-processing the image we sent that image to MTCNN to detect the faces. After detecting the faces we are sending those faces for a feature extraction model to get embeddings.","title":"Concluding Remarks "},{"location":"face-detection/#contributors","text":"1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Phani Tulasi Batchu 4. Pendyala Sri Harshita","title":"Contributors"},{"location":"face-detection/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"face-recognition/","text":"body{ background-color:rgb(34, 32, 32); } Face Recognition Inception-ResNet-v2 ResNet and Inception have been central to the largest advances in image recognition performance in recent years, with very good performance at a relatively low computational cost. Inception-ResNet combines the Inception architecture, with residual connections. Inception Resnet (V1) is a model in facenet-pytorch library which is pretrained on VGGFace2 and CASIA-Webface. Facenet FaceNet is a one-shot face recognition pipeline, it uses a deep convolutional network that directly learns a mapping from face images to a compact Euclidean multidimensional space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet compact 128-D embeddings as feature vectors. To train, they used triplets of roughly aligned matching / non-matching face patches. A triplet is nothing but a collection one anchor image, one matching image to the anchor image and one non-matching image to the anchor image. So the triplet loss minimises the distance between an anchor and a positive, both of which have the same identity, and maximises the distance between the anchor and a negative of a different identity. In this step we encode the image into embeddings. Embeddings are high dimensional vectors. For feature extraction we are using a Facenet model. FaceNet is a deep neural network used for extracting features from an image of a person\u2019s face. It was published in 2015 by Google researchers that FaceNet takes an image of the person\u2019s face as input and outputs a vector of 128 numbers which represent the most important features of a face. In machine learning, this vector is called embedding. Basically, FaceNet takes a person\u2019s face and compresses it into a vector of 128 numbers. Ideally, embeddings of similar faces are also similar. With the help of those embeddings, we perform face classification. In this project we used facenet to get the 128-d embeddings from each of the input images. Contributors 1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Face recognition"},{"location":"face-recognition/#face-recognition","text":"","title":"Face Recognition"},{"location":"face-recognition/#inception-resnet-v2","text":"ResNet and Inception have been central to the largest advances in image recognition performance in recent years, with very good performance at a relatively low computational cost. Inception-ResNet combines the Inception architecture, with residual connections. Inception Resnet (V1) is a model in facenet-pytorch library which is pretrained on VGGFace2 and CASIA-Webface.","title":"Inception-ResNet-v2"},{"location":"face-recognition/#facenet","text":"FaceNet is a one-shot face recognition pipeline, it uses a deep convolutional network that directly learns a mapping from face images to a compact Euclidean multidimensional space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet compact 128-D embeddings as feature vectors. To train, they used triplets of roughly aligned matching / non-matching face patches. A triplet is nothing but a collection one anchor image, one matching image to the anchor image and one non-matching image to the anchor image. So the triplet loss minimises the distance between an anchor and a positive, both of which have the same identity, and maximises the distance between the anchor and a negative of a different identity. In this step we encode the image into embeddings. Embeddings are high dimensional vectors. For feature extraction we are using a Facenet model. FaceNet is a deep neural network used for extracting features from an image of a person\u2019s face. It was published in 2015 by Google researchers that FaceNet takes an image of the person\u2019s face as input and outputs a vector of 128 numbers which represent the most important features of a face. In machine learning, this vector is called embedding. Basically, FaceNet takes a person\u2019s face and compresses it into a vector of 128 numbers. Ideally, embeddings of similar faces are also similar. With the help of those embeddings, we perform face classification. In this project we used facenet to get the 128-d embeddings from each of the input images.","title":"Facenet"},{"location":"face-recognition/#contributors","text":"1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh","title":"Contributors"},{"location":"face-recognition/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"face-verification/","text":"body{ background-color:rgb(34, 32, 32); } Face classification In this step with the help of embeddings we are identifying the person in the image. There are many classification algorithms like SVC, Logistic regression, KNN etc. We are using SVC, cosine similarity and Euclidean distance approach for classifying. In cosine similarity we will find the cosine angle between two vectors. If the angle is less than the two images are the same. In the Euclidean approach we are finding the Euclidean distance between two vectors. If the distance is less than the both images are the same. SVM Support Vector Machine\u201d (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However, it is mostly used in classification, where it classifies data into different classes. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features, we have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well. Support Vector Machines Applied to Face Recognition A SVM algorithm generates a decision surface separating the two classes. For face recognition, we re-interpret the decision surface to produce a similarity metric between two facial images. This constructs face-recognition algorithms. EUCLEDIAN Distance Euclidean Distance represents the shortest distance between two points(or) the straight line distance. To find the two points on a plane, the length of a segment connecting the two points is measured. We derive the Euclidean distance formula using the Pythagoras theorem. Let us assume that (x1,y1) and (x2,y2) are two points in a two-dimensional plane. Here is the Euclidean distance formula, where 'd' is the distance: - COSINE Similarity Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity. Contributors 1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Pendyala Sri Harshita 4. Solipuram Akshith Reddy ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Face verification"},{"location":"face-verification/#face-classification","text":"In this step with the help of embeddings we are identifying the person in the image. There are many classification algorithms like SVC, Logistic regression, KNN etc. We are using SVC, cosine similarity and Euclidean distance approach for classifying. In cosine similarity we will find the cosine angle between two vectors. If the angle is less than the two images are the same. In the Euclidean approach we are finding the Euclidean distance between two vectors. If the distance is less than the both images are the same.","title":"Face classification"},{"location":"face-verification/#svm","text":"Support Vector Machine\u201d (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However, it is mostly used in classification, where it classifies data into different classes. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features, we have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.","title":"SVM"},{"location":"face-verification/#support-vector-machines-applied-to-face-recognition","text":"A SVM algorithm generates a decision surface separating the two classes. For face recognition, we re-interpret the decision surface to produce a similarity metric between two facial images. This constructs face-recognition algorithms.","title":"Support Vector Machines Applied to Face Recognition"},{"location":"face-verification/#eucledian-distance","text":"Euclidean Distance represents the shortest distance between two points(or) the straight line distance. To find the two points on a plane, the length of a segment connecting the two points is measured. We derive the Euclidean distance formula using the Pythagoras theorem. Let us assume that (x1,y1) and (x2,y2) are two points in a two-dimensional plane. Here is the Euclidean distance formula, where 'd' is the distance: -","title":"EUCLEDIAN Distance"},{"location":"face-verification/#cosine-similarity","text":"Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.","title":"COSINE Similarity"},{"location":"face-verification/#contributors","text":"1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Pendyala Sri Harshita 4. Solipuram Akshith Reddy","title":"Contributors"},{"location":"face-verification/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"introduction/","text":"body{ background-color:rgb(34, 32, 32); } Output Image Flow Chart Conclusion Hereby we conclude that marking attendance using Facial Recognition model is a smatter way of doing it, as it is more user friendly as well as accurate. Contributors 1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Phani Tulasi Batchu 4. Solipuram Akshith Reddy 5. Pendyala Sri Harshita ----TABLE OF CONTENT---- 1. Home 2. Overview 4. Data-pre-processing 5. Face-Detection 6. Face-Alignment 7. Face-Recognition 8. Face-Verification 9. Conclusion","title":"Introduction"},{"location":"introduction/#output-image","text":"","title":"Output Image"},{"location":"introduction/#flow-chart","text":"","title":"Flow Chart"},{"location":"introduction/#conclusion","text":"Hereby we conclude that marking attendance using Facial Recognition model is a smatter way of doing it, as it is more user friendly as well as accurate.","title":"Conclusion"},{"location":"introduction/#contributors","text":"1. Meda Sai Balaji Bhargav 2. Kodumuru Saketh 3. Phani Tulasi Batchu 4. Solipuram Akshith Reddy 5. Pendyala Sri Harshita","title":"Contributors"},{"location":"introduction/#-table-of-content-","text":"1. Home 2. Overview 4. Data-pre-processing 5. Face-Detection 6. Face-Alignment 7. Face-Recognition 8. Face-Verification 9. Conclusion","title":"----TABLE OF CONTENT----"},{"location":"overview/","text":"body{ background-color:rgb(34, 32, 32); } Purpose The prime purpose of doing this project is that we came across some deceiving practices as well as complications in the matter of marking attendance, which drove us towards an idea of using technology to solve this real world problem. Our major use case is Government schools and colleges which aim at providing aid, education and food to lakhs of students at free of cost. But there are many deceiving practices done by various officials, who misuse the benefits meant for those kids. And it is all possible by manipulating the attendance and getting the more out of it. So this is our attempt to turn down such actions, wherein we will be automating the attendance procedure by just needing the group image of students present in each class. Vision We have done end-to-end face recognition which is the soul of this project. And we used Amazon EC2 cloud services for configuring and deploying our project. A GUI is built which forms the home screen wherein the user needs to upload six different images of all the students in each and every class. After the upload is complete, the images are loaded into a particular folder and sent for training into the model. The model extracts a 128-d vector containing the facial landmarks in each face and stores it in a Database. Now a group image from which the attendance has to be taken is once again uploaded in the GUI, the given image is processed against the model by comparing the face embeddings of each face in the group image to that in the database and the output marks the attendance of the whole class which is again stored in a new database. We used MySQL for the database. This process can be iterated for different classes everyday making the attendance marking process easy and accurate. ----TABLE OF CONTENT---- 1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"Overview"},{"location":"overview/#purpose","text":"The prime purpose of doing this project is that we came across some deceiving practices as well as complications in the matter of marking attendance, which drove us towards an idea of using technology to solve this real world problem. Our major use case is Government schools and colleges which aim at providing aid, education and food to lakhs of students at free of cost. But there are many deceiving practices done by various officials, who misuse the benefits meant for those kids. And it is all possible by manipulating the attendance and getting the more out of it. So this is our attempt to turn down such actions, wherein we will be automating the attendance procedure by just needing the group image of students present in each class.","title":"Purpose"},{"location":"overview/#vision","text":"We have done end-to-end face recognition which is the soul of this project. And we used Amazon EC2 cloud services for configuring and deploying our project. A GUI is built which forms the home screen wherein the user needs to upload six different images of all the students in each and every class. After the upload is complete, the images are loaded into a particular folder and sent for training into the model. The model extracts a 128-d vector containing the facial landmarks in each face and stores it in a Database. Now a group image from which the attendance has to be taken is once again uploaded in the GUI, the given image is processed against the model by comparing the face embeddings of each face in the group image to that in the database and the output marks the attendance of the whole class which is again stored in a new database. We used MySQL for the database. This process can be iterated for different classes everyday making the attendance marking process easy and accurate.","title":"Vision"},{"location":"overview/#-table-of-content-","text":"1. Overview 3. Data-pre-processing 4. Face-Detection 5. Face-Alignment 6. Face-Recognition 7. Face-Verification 2. Conclusion","title":"----TABLE OF CONTENT----"}]}